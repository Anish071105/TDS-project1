# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.


"""

import requests
import pandas as pd
import time

# Constants
GITHUB_API_URL = "https://api.github.com"
CITY = "Melbourne"
MIN_FOLLOWERS = 100
TOKEN = "..."#Replace with token since i had o remove it as it was sensiitve info 

# Function to check rate limit status
def check_rate_limit():
    response = requests.get(f"{GITHUB_API_URL}/rate_limit", headers={"Authorization": f"token {TOKEN}"})
    if response.status_code == 200:
        rate_info = response.json()
        print("Rate Limit Info:", rate_info)
    else:
        print("Failed to fetch rate limit info:", response.status_code, response.text)

# Function to fetch users
def fetch_users():
    users = []
    page = 1

    while True:
        response = requests.get(
            f"{GITHUB_API_URL}/search/users",
            params={"q": f"location:{CITY} followers:>{MIN_FOLLOWERS}", "page": page},
            headers={"Authorization": f"token {TOKEN}"}
        )

        if response.status_code != 200:
            print("Error:", response.status_code)
            print("Response content:", response.text)  # Print the error message or HTML content
            break

        data = response.json()

        if "items" not in data or len(data["items"]) == 0:
            break

        for item in data["items"]:
            user_info = {
                "login": item["login"],
                "url": item["url"]  # Store the URL to fetch detailed user info later
            }
            users.append(user_info)

        page += 1

    return users

# Function to fetch detailed information for a user
def fetch_user_details(username):
    response = requests.get(
        f"{GITHUB_API_URL}/users/{username}",
        headers={"Authorization": f"token {TOKEN}"}
    )

    if response.status_code == 200:
        user_data = response.json()
        return {
            "login": user_data.get("login"),
            "name": user_data.get("name", ""),
            "company": user_data.get("company", "").strip('@').upper().strip() if user_data.get("company") else "",
            "location": user_data.get("location", ""),
            "email": user_data.get("email", ""),
            "hireable": user_data.get("hireable", ""),
            "bio": user_data.get("bio", ""),
            "public_repos": user_data.get("public_repos", 0),
            "followers": user_data.get("followers", 0),
            "following": user_data.get("following", 0),
            "created_at": user_data.get("created_at", "")
        }
    else:
        print(f"Failed to fetch details for {username}: {response.status_code} {response.text}")
        return {}  # Return an empty dictionary instead of None

# Main execution
check_rate_limit()  # Check the rate limit before starting

users = fetch_users()
detailed_users = []

for user in users:
    user_details = fetch_user_details(user["login"])
    if user_details:  # This will always be true since we return an empty dict instead of None
        detailed_users.append(user_details)

    # Sleep for a short duration to avoid hitting the rate limit
    time.sleep(1)  # Sleep for 1 second between requests

# Save to CSV
users_df = pd.DataFrame(detailed_users)
users_df.to_csv('users.csv', index=False)

print("User  data saved to 'users.csv'.")

#Function to download users.csv
from google.colab import files
files.download('users.csv')

import requests
import pandas as pd
import time

# Constants
GITHUB_API_URL = "https://api.github.com"
TOKEN = "..."#Replace with token since i had o remove it as it was sensiitve info 

# Function to fetch repositories for a given user
def fetch_user_repositories(username):
    repositories = []
    page = 1

    while True:
        response = requests.get(
            f"{GITHUB_API_URL}/users/{username}/repos",
            params={"sort": "pushed", "direction": "desc", "per_page": 100, "page": page},
            headers={"Authorization": f"token {TOKEN}"}
        )

        if response.status_code != 200:
            print(f"Error fetching repositories for {username}: {response.status_code} {response.text}")
            break

        repos_data = response.json()

        if not repos_data:
            break  # No more repositories to fetch

        for repo in repos_data:
            repo_info = {
                "login": username,
                "full_name": repo.get("full_name"),
                "created_at": repo.get("created_at"),
                "stargazers_count": repo.get("stargazers_count", 0),
                "watchers_count": repo.get("watchers_count", 0),
                "language": repo.get("language"),
                "has_projects": repo.get("has_projects", False),
                "has_wiki": repo.get("has_wiki", False),
                "license_name": repo.get("license", {}).get("name") if repo.get("license") else None
            }
            repositories.append(repo_info)

            # Stop if we have collected 500 repositories
            if len(repositories) >= 500:
                return repositories[:500]

        page += 1
        time.sleep(1)  # Sleep to avoid hitting the rate limit

    return repositories

# Main execution
users_df = pd.read_csv('users.csv')  # Load the users from the CSV file
all_repositories = []

for index, row in users_df.iterrows():
    username = row["login"]
    print(f"Fetching repositories for {username}...")
    user_repositories = fetch_user_repositories(username)
    all_repositories.extend(user_repositories)

# Save to CSV-
repositories_df = pd.DataFrame(all_repositories)
repositories_df.to_csv('repositories.csv', index=False)

print("Repositories data saved to 'repositories.csv'.")

#Function to obtain repositories
from google.colab import files
files.download('repositories.csv')

print(repositories_df)
print(users_df)

#code to find 5 most users with highest followers
top_followers = users_df[users_df['location'].str.contains("Melbourne", na=False)].nlargest(5, 'followers')
top_followers_logins = ','.join(top_followers['login'])
print(top_followers_logins)

#code to find 5 earliest users
earliest_users = users_df.sort_values(by='created_at').head(5)
earliest_users_logins = ','.join(earliest_users['login'].tolist())
print("The 5 earliest registered GitHub users in Melbourne are:", earliest_users_logins)

#code to find most popular licenses
popular_licenses = repositories_df['license_name'].dropna().value_counts().head(3).index.tolist()
print("Most popular licenses:", ",".join(popular_licenses))

#code to find company with most users
company_dict = {}
for company in users_df['company'].dropna():
    if company in company_dict:
        company_dict[company] += 1
    else:
        company_dict[company] = 1
max_count = 0
company_max = ""
for company in company_dict:
    if company_dict[company] > max_count:
        max_count = company_dict[company]
        company_max = company
print("Company with the most users:", company_max)

#code to find the languages used before 2020
recent_users = users_df[pd.to_datetime(users_df['created_at']) < '2020']['login']
recent_repos = repositories_df[repositories_df['login'].isin(recent_users)]
# Initialize an empty dictionary to count languages
language_dict = {}
# Count occurrences of each language in the filtered recent_repos DataFrame
for language in recent_repos['language'].dropna():
    if language in language_dict:
        language_dict[language] += 1
    else:
        language_dict[language] = 1
# Find the first and second most popular languages
max_count = 0
second_max_count = 0
language_max = ""
second_language_max = ""
values=0
for language in language_dict:
    values+=language_dict[language]
    if language_dict[language] > max_count:
        second_max_count = max_count
        second_language_max = language_max
        max_count = language_dict[language]
        language_max = language
    elif language_dict[language] > second_max_count:
        second_max_count = language_dict[language]
        second_language_max = language
print("Second most popular language among users who joined after 2020:", second_language_max)
print(language_max)
print(language_dict[language_max])
print(values)
language_dict[second_language_max]

#code to find the languages used after 2020
recent_users = users_df[pd.to_datetime(users_df['created_at']) > '2020']['login']
recent_repos = repositories_df[repositories_df['login'].isin(recent_users)]

# Initialize an empty dictionary to count languages
language_dict = {}

# Count occurrences of each language in the filtered recent_repos DataFrame
for language in recent_repos['language'].dropna():
    if language in language_dict:
        language_dict[language] += 1
    else:
        language_dict[language] = 1

# Find the first and second most popular languages
max_count = 0
second_max_count = 0
language_max = ""
second_language_max = ""
values=0
for language in language_dict:
    values+=language_dict[language]
    if language_dict[language] > max_count:
        second_max_count = max_count
        second_language_max = language_max
        max_count = language_dict[language]
        language_max = language
    elif language_dict[language] > second_max_count:
        second_max_count = language_dict[language]
        second_language_max = language
print("Second most popular language among users who joined after 2020:", second_language_max)
print(language_max)
print(language_dict[language_max])
print(values)
language_dict[second_language_max]
language_stats = repositories_df.dropna(subset=['language']).groupby('language')['stargazers_count'].agg(['sum', 'count'])

# Calculate average stars per repository for each language
language_stats['average_stars'] = language_stats['sum'] / language_stats['count']
# Find the language with the highest average stars
language_max_avg_stars = language_stats['average_stars'].idxmax()
print("Language with the highest average number of stars per repository:", language_max_avg_stars)

#code to find highest leader_strength
leader_strength_dict = {}
for index, row in users_df.iterrows():
        leader_strength = row['followers'] / (1 + row['following'])
        leader_strength_dict[row['login']] = leader_strength
top_leaders = sorted(leader_strength_dict, key=leader_strength_dict.get)[:5:-1]
top_leaders_str = ','.join(top_leaders)
print("Top 5 users in terms of leader strength:", top_leaders_str)

#Code to find correlations
correlation = users_df['followers'].corr(users_df['public_repos'])
print("Correlation between followers and public repositories:", round(correlation, 3))

#Code to plot public repos vs followers
import statsmodels.api as sm
X = users_df['public_repos']
y = users_df['followers']
sns.set(style="whitegrid")
# Create a scatter plot for followers vs. public repos
plt.figure(figsize=(10, 6))
sns.scatterplot(data=users_df, x='public_repos', y='followers')
# Title and labels
plt.title('Followers vs. Public Repositories ', fontsize=16)
plt.xlabel('Number of Public Repositories', fontsize=14)
plt.ylabel('Number of Followers', fontsize=14)
plt.grid()
# Show plot
plt.tight_layout()
plt.show()

#Code to find correlation
X = sm.add_constant(X)
model = sm.OLS(y, X).fit()
slope = model.params['public_repos']
print("Regression slope of followers on repos:", round(slope, 3))

#Code to find wiki enabled and project enabled repos
repo_features_df = repositories_df[['has_projects', 'has_wiki']]
correlation = repo_features_df.corr().loc['has_projects', 'has_wiki']
correlation_rounded = round(correlation, 3)
correlation_rounded

#Code to find average of hireabel-average of rest 
users_df['hireable'].fillna(False)
hireable_users = users_df[users_df['hireable'] == True]
non_hireable_users = users_df[users_df['hireable'] == False]
average_following_hireable = hireable_users['following'].mean()
average_following_non_hireable = non_hireable_users['following'].mean()
difference = average_following_hireable - average_following_non_hireable
difference_rounded = round(difference, 3)
difference_rounded

#Code to find slope between repos and followers
import statsmodels.api as sm
users_with_bio = users_df[users_df['bio'].notna()]
users_with_bio['bio_length'] = users_with_bio['bio'].str.split().str.len()
X = users_with_bio['bio_length']  # Independent variable
y = users_with_bio['followers']
# Add a constant to the independent variable for the intercept
X = sm.add_constant(X)
# Fit the regression model
model = sm.OLS(y, X).fit()
# Get the slope (coefficient for bio_length)
slope = model.params['bio_length']
# Print the slope rounded to 3 decimal places
print("Regression slope of followers on bio word count:", round(slope, 3))

repositories_df['created_at'] = pd.to_datetime(repositories_df['created_at'])
# Filter repositories created on weekends (Saturday: 5, Sunday: 6)
weekend_repositories = repositories_df[repositories_df['created_at'].dt.dayofweek >= 5]
# Count the number of repositories created by each user
top_users = weekend_repositories['login'].value_counts().head(5)
# Get the top 5 user logins in order
top_user_logins = top_users.index.tolist()
# Print the top 5 user logins as a comma-separated string
print("Top 5 users who created the most repositories on weekends:", ",".join(top_user_logins))

# Fill NaN values in 'hireable' with False to treat them as non-hireable
users_df['hireable'] = users_df['hireable'].fillna(False)
# Calculate the fraction of hireable users with email addresses
hireable_with_email = users_df[users_df['hireable'] == True]['email'].notna().sum()
total_hireable_users = users_df['hireable'].value_counts().get(True, 0)
fraction_hireable = hireable_with_email / total_hireable_users if total_hireable_users > 0 else 0
# Calculate the fraction of non-hireable users with email addresses
non_hireable_with_email = users_df[users_df['hireable'] == False]['email'].notna().sum()
total_non_hireable_users = users_df['hireable'].value_counts().get(False, 0)
fraction_non_hireable = non_hireable_with_email / total_non_hireable_users if total_non_hireable_users > 0 else 0
# Calculate the difference
email_fraction_difference = fraction_hireable - fraction_non_hireable
# Print the result rounded to 3 decimal places
print("Fraction of email sharing difference (hireable - non-hireable):", round(email_fraction_difference, 3))

#Code to find surnames
users_with_names = users_df[users_df['name'].notna()].copy()  # Use .copy() to avoid the warning
users_with_names.loc[:, 'surname'] = users_with_names['name'].str.split().str[-1]  # Explicit assignment
# Count occurrences of each surname
surname_counts = users_with_names['surname'].value_counts()
# Find the maximum count
max_count = surname_counts.max()
# Get the most common surnames (there could be ties)
most_common_surnames = surname_counts[surname_counts == max_count].index.tolist()
# Sort the surnames alphabetically
most_common_surnames.sort()
# Print the most common surname(s) as a comma-separated string
print("Most common surname(s):", ",".join(most_common_surnames))

#Rewrite for hireable email
hireable_with_email_fraction = users_df[users_df['hireable'] == True]['email'].notna().mean()
non_hireable_with_email_fraction = users_df[~(users_df['hireable'] == True)]['email'].notna().mean()
difference = round(hireable_with_email_fraction - non_hireable_with_email_fraction, 3)
print(difference)


#To plot graph between repos and language to get estimate of language before amd after 2020(Used in observation of readme)
import pandas as pd
import matplotlib.pyplot as plt
before_2020 = repositories_df[repositories_df['created_at'] < '2020-01-01']
after_2020 = repositories_df[repositories_df['created_at'] >= '2020-01-01']
lang_before_2020 = before_2020['language'].value_counts().nlargest(10)
lang_after_2020 = after_2020['language'].value_counts().nlargest(10)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
lang_before_2020.plot(kind='bar', ax=ax1)
ax1.set_title('Top 10 Programming Languages Before 2020')
ax1.set_xlabel('Language')
ax1.set_ylabel('Number of Repositories')
lang_after_2020.plot(kind='bar', ax=ax2)
ax2.set_title('Top 10 Programming Languages After 2020')
ax2.set_xlabel('Language')
ax2.set_ylabel('Number of Repositories')
plt.tight_layout()
plt.show()